{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load into Data frane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/imdb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           review\n",
      "sentiment        \n",
      "negative    25000\n",
      "positive    25000\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "print(df.groupby('sentiment').count())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Unwanted characters, and try to use only the first 15000 rows of the data to speed up our fine-tunning.\n",
    "- remove punctuation marks\n",
    "- remove characters which are not letters or digits\n",
    "- remove successive whitespaces\n",
    "- convert the text to lower case\n",
    "- strip whitespaces from the beginning and the end of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(15000)\n",
    "df.sentiment = [1 if s == 'positive' else 0 for s in df.sentiment]\n",
    "def process(x):\n",
    "    x = re.sub('[,\\.!?:()\"]', '', x)\n",
    "    x = re.sub('<.*?>', ' ', x)\n",
    "    x = re.sub('http\\S+', ' ', x)\n",
    "    x = re.sub('[^a-zA-Z0-9]', ' ', x)\n",
    "    x = re.sub('\\s+', ' ', x)\n",
    "    return x.lower().strip()\n",
    "\n",
    "df['review'] = df['review'].apply(lambda x: process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production the filming tech...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there s a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei s love in the time of money is a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  one of the other reviewers has mentioned that ...          1\n",
       "1  a wonderful little production the filming tech...          1\n",
       "2  i thought this was a wonderful way to spend ti...          1\n",
       "3  basically there s a family where a little boy ...          0\n",
       "4  petter mattei s love in the time of money is a...          1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data distribution over the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the number of sentences for each sentiment category\n",
    "positive_counts = len(df[df.sentiment == 1])\n",
    "negative_counts = len(df[df.sentiment == 0])\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(['Positive', 'Negative'], [positive_counts, negative_counts], color=['green', 'red'])\n",
    "plt.title('Distribution of Sentence Counts by Sentiment')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Sentence Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess our data, padding them (masking them appropriately)\n",
    "Let’s first see how the BERT tokenizer converts sentences into token ids.\n",
    "token ids : it is an integer that represents a particular token.\n",
    "\n",
    "attention mask : it is a sequence of ones and zeroes to tell the model which token comes from input sentence (segment id =1) and which are just padding token(segment id =0).\n",
    "\n",
    "padding : when we train BERT model we make sure that every input to the model should have same size that means same length of inputs so that the model can perform back propagation efficiently but all our input which are review text can not be in same size, some can be small review and some can be large. Padding is the technique where we make our entire review in the same size but first we have to decide a fixed length or max_length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function tokenizer.tokenize() and tokenizer.convert_tokens_to_ids() are general purpose functions and it does not convert our review compatible for training data like adding [CLS] token at beginning, padding and [SEP] token at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000,) (3000,)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "train_sentences = train.review.values\n",
    "train_labels = train.sentiment.values\n",
    "test_sentences = test.review.values\n",
    "test_labels = test.sentiment.values\n",
    "\n",
    "print(train_sentences.shape, test_sentences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERTMODEL = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERTMODEL,\n",
    "                                          do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging face library provides another function called tokenizer.encode_plus() which we will use to perform almost entire preprocessing steps in one go. It\n",
    "\n",
    "- converts reviews into tokens\n",
    "- adds [CLS] token at the beginning of input\n",
    "- performs padding if sequence length is less than max_len\n",
    "- performs truncation if sequence length is greater than max_len\n",
    "- adds [SEP] token at the end of sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(data,labels):\n",
    "  input_ids = []\n",
    "  attention_masks = []\n",
    "\n",
    "  for sent in data:\n",
    "      # `encode_plus` will:\n",
    "      #   (1) Tokenize the sentence.\n",
    "      #   (2) Prepend the `[CLS]` token to the start.\n",
    "      #   (3) Append the `[SEP]` token to the end.\n",
    "      #   (4) Map tokens to their IDs.\n",
    "      #   (5) Pad or truncate the sentence to `max_length`\n",
    "      #   (6) Create attention masks for [PAD] tokens.\n",
    "      encoded_dict = tokenizer.encode_plus(\n",
    "                          sent,                      # Sentence to encode.\n",
    "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                          max_length = 64,           # Pad & truncate all sentences.\n",
    "                          pad_to_max_length = True,\n",
    "                          truncation=True,\n",
    "                          return_attention_mask = True,   # Construct attn. masks.\n",
    "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                    )\n",
    "      \n",
    "      # Add the encoded sentence to the list.    \n",
    "      input_ids.append(encoded_dict['input_ids'])\n",
    "      \n",
    "      # And its attention mask (simply differentiates padding from non-padding).\n",
    "      attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "  # Convert the lists into tensors.\n",
    "  input_ids = torch.cat(input_ids, dim=0)\n",
    "  attention_masks = torch.cat(attention_masks, dim=0)\n",
    "  labels = torch.tensor(labels)\n",
    "\n",
    "  return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, each sentece;\n",
    "\n",
    "- Tokenized\n",
    "- [CLS] Token added at the beginning\n",
    "- [SEP] Token added at the end\n",
    "- Padded or truncated to a fixed length of tokens, I choosed 64 for this example\n",
    "- Attention masks generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zq/m7j0g5vs1b32pfht2xtpnw4m0000gn/T/ipykernel_55678/4219197917.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  my husband and i enjoy the doodlebops as much as our 8 month old baby does we have bought him dvd s and cd s just so we can watch and listen to them ourselves they are fun energetic and very entertaining they encourage children to be active share and care they always have a positive message along with fun entertainment every time our son hears the theme song he quickly turns his head toward the television and starts bouncing up and down in excitement dee dee is a wonderful singer she has a great voice moe is a great dancer i would recommend the doodlebops to anyone with children our favorite song is the bird song you just can not help but smile and want to dance when you hear it\n",
      "Token IDs: tensor([  101,  2026,  3129,  1998,  1045,  5959,  1996, 20160, 10362,  5092,\n",
      "         4523,  2004,  2172,  2004,  2256,  1022,  3204,  2214,  3336,  2515,\n",
      "         2057,  2031,  4149,  2032,  4966,  1055,  1998,  3729,  1055,  2074,\n",
      "         2061,  2057,  2064,  3422,  1998,  4952,  2000,  2068,  9731,  2027,\n",
      "         2024,  4569, 18114,  1998,  2200, 14036,  2027,  8627,  2336,  2000,\n",
      "         2022,  3161,  3745,  1998,  2729,  2027,  2467,  2031,  1037,  3893,\n",
      "         4471,  2247,  2007,   102])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_input_ids, train_attention_masks,train_labels = generate_data(train_sentences,train_labels)\n",
    "test_input_ids, test_attention_masks,test_labels = generate_data(test_sentences,test_labels)\n",
    "\n",
    "print('Original: ', train_sentences[0])\n",
    "print('Token IDs:', train_input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the BertForSequenceClassification\n",
    " This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bert Model Output Overview\n",
    "\n",
    "- last_hidden_state : It is the first output we get from the model and as its name it is the output from last layer. The size of this output will be (no. of batches , no. of tokens in each batch, size of hidden layer). in our case we have only 16 batch with 50 token and for BERT-base size of the hidden layer is 768 so the size of last hidden state (16,50,768) can be seen in the code above.\n",
    "\n",
    "- Pooler Output : Creator of BERT model said that pooler_output which corresponds to the last hidden state of [CLS] token does not have any interpretability but it is best choice as an input for a classifier that can be fine tuned on a separate dataset. The main reason behind this is the way it pretrained. We must not forget that we are using the BERT model as transfer learning which is already pre trained on large amounts of data. [CLS] token is always used as a starting token while performing pretraing using a masked language model and next sentence prediction that’s why a pooler output which itself doesn’t have any interpretability but it captures all the information for a particular input.\n",
    "\n",
    "- Hidden_state : One good reason to love this model is its hidden state. BERT application is not limited to using pooler output to fine tune the classifier but one can also explore the advantages by using its hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "                                                      \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "                                                        num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                                                                        # You can increase this for multi-class tasks.   \n",
    "                                                        output_attentions = False, # Whether the model returns attentions weights.\n",
    "                                                        output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tango.tew/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, \n",
    "                  eps = 1e-8)\n",
    "                  \n",
    "epochs = 4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create functions for calculating training time and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "    \n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model and data are ready, we can start the training.\n",
    "\n",
    "- Unpack our data inputs and labels\n",
    "- We need to feed model with input_id, attentions mask and labels.\n",
    "- Load data onto the GPU for acceleration\n",
    "- Clear out the gradients calculated in the previous pass.\n",
    "- Forward pass (feed input data through the network)\n",
    "- Backward pass (backpropagation)\n",
    "- Tell the network to update parameters with optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    375.    Elapsed: 0:09:03.\n",
      "  Batch    80  of    375.    Elapsed: 0:19:31.\n",
      "  Batch   120  of    375.    Elapsed: 0:30:11.\n",
      "  Batch   160  of    375.    Elapsed: 0:40:02.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Perform a backward pass to calculate the gradients.\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Clip the norm of the gradients to 1.0.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# This is to help prevent the \"exploding gradients\" problem.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "       \n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0]\n",
    "        b_input_mask = batch[1]\n",
    "        b_labels = batch[2]\n",
    "\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        result = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels,\n",
    "                       return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,   \n",
    "            'Training Time': training_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Our Bert model trained for 4 epochs, let’s test it.\n",
    "\n",
    "- Unpack our data inputs and labels\n",
    "- Load data onto the GPU for acceleration\n",
    "- Forward pass, feeding input data through the network\n",
    "- Compute loss on our validation data and track variables for monitoring progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in test_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions.\n",
    "      result = model(b_input_ids, \n",
    "                     token_type_ids=None, \n",
    "                     attention_mask=b_input_mask,\n",
    "                     return_dict=True)\n",
    "\n",
    "  logits = result.logits\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "prediction_set = []\n",
    "\n",
    "for i in range(len(true_labels)):\n",
    "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "  prediction_set.append(pred_labels_i)\n",
    "\n",
    "prediction_scores = [item for sublist in prediction_set for item in sublist]\n",
    "\n",
    "print('Preds are ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score = f1_score(test_labels, prediction_scores, average='macro')\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "- Most of the sentences are longer than 64 words, setting max lenght to higher number will require more calculation.\n",
    "- Increasing epoch number\n",
    "- Using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning and testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the appropriate max length\n",
    "The best way to decide max_len for our task is plotting a distribution graph for the number of words in each review. We will assign max_len to that value for which most of the review is covered without truncation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2470\n"
     ]
    }
   ],
   "source": [
    "lengths = np.array([len(x.split()) for x in df['review']])\n",
    "print(lengths.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tango.tew/miniconda3/envs/pytorch/lib/python3.8/site-packages/seaborn/axisgrid.py:123: UserWarning: The figure layout has changed to tight\n",
      "  self._figure.tight_layout(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(-3.9305555555555554, 0.5, 'Sentence Length')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.title('Max Length Distribution')\n",
    "sns.displot(lengths)\n",
    "plt.xlabel('Counts')\n",
    "plt.ylabel('Sentence Length')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Training Function\n",
    "The function below trains the model (forward pass and backward pass) for one epoch and also calculate accuracy and average loss for an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, criterion, optimizer, scheduler, BATCH_SIZE, MAX_LEN, n_examples):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for s in data_loader:\n",
    "        input_ids = s['input_ids']\n",
    "        attention_mask = s['attention_mask']\n",
    "        targets = s['target']\n",
    "\n",
    "        #forward props\n",
    "        predictions = model(input_ids, attention_mask)\n",
    "        loss = criterion(predictions, targets)\n",
    "        _, pred_classes = torch.max(predictions, dim=1)\n",
    "\n",
    "        #back propagate\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # training loss and number of predictions(accuracy)\n",
    "        train_loss.append(loss.item())\n",
    "        correct_predictions += torch.sum(pred_classes == targets)\n",
    "\n",
    "        return correct_predictions.double() / n_examples, np.mean(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation function\n",
    "This fuction evaluate model performance on evaluation data for every epoch by calculation accuracy and average loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, criterion, BATCH_SIZE, MAX_LEN, n_examples):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for s in data_loader:\n",
    "            input_ids = s['input_ids']\n",
    "            attention_mask = s['attention_mask']\n",
    "            targets = s['target']\n",
    "\n",
    "            #forward props\n",
    "            predictions = model(input_ids, attention_mask)\n",
    "            loss = criterion(predictions, targets)\n",
    "            _, pred_classes = torch.max(predictions, dim=1)\n",
    "\n",
    "            # validation loss and number of predictions(accuracy)\n",
    "            val_loss.append(loss.item())\n",
    "            correct_predictions += torch.sum(pred_classes == targets)\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Train and Evaluate Model\n",
    "In this loop we are calling functions train_model() and eval_model() and printing the accuracy and loss for both training and validation data for every epoch and also saving the model if model performance on validation data improves than the performance in previous epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m train_acc, train_loss \u001b[38;5;241m=\u001b[39m train_model(model, train_loader, criterion, optimizer, scheduler, BATCH_SIZE, max_length, \u001b[38;5;28mlen\u001b[39m(training_dataset))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#validation\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m val_acc, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Saving the training logs\u001b[39;00m\n\u001b[1;32m     15\u001b[0m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_acc)\n",
      "Cell \u001b[0;32mIn[40], line 13\u001b[0m, in \u001b[0;36meval_model\u001b[0;34m(model, data_loader, criterion, BATCH_SIZE, MAX_LEN, n_examples)\u001b[0m\n\u001b[1;32m     10\u001b[0m targets \u001b[38;5;241m=\u001b[39m s[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#forward props\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, targets)\n\u001b[1;32m     15\u001b[0m _, pred_classes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(predictions, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[36], line 10\u001b[0m, in \u001b[0;36mSentimentClassifier.forward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[0;32m---> 10\u001b[0m     temp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m temp[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     12\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(pooled_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1015\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1016\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1017\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[0;32m-> 1022\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1035\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    603\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    605\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 612\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:365\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    363\u001b[0m     attention_probs \u001b[38;5;241m=\u001b[39m attention_probs \u001b[38;5;241m*\u001b[39m head_mask\n\u001b[0;32m--> 365\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    368\u001b[0m new_context_layer_shape \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size,)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "# EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    model = SentimentClassifier(num_classes)\n",
    "    train_acc, train_loss = train_model(model, train_loader, criterion, optimizer, scheduler, BATCH_SIZE, max_length, len(training_dataset))\n",
    "\n",
    "    #validation\n",
    "    val_acc, val_loss = eval_model(model, val_loader, criterion, BATCH_SIZE, max_length, len(val_dataset))\n",
    "\n",
    "    # Saving the training logs\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    print(f'Training Loss: {train_loss}, Training Accuracy: {train_acc}, Validation Loss: {val_loss}, Validation Accuracy: {val_acc}')\n",
    "\n",
    "    # Saving the best model\n",
    "    if val_acc > best_accuracy:\n",
    "        best_model = f'best_model_state{val_acc}.bin'\n",
    "        torch.save(model.state_dict(), best_model)\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way I printed epochs ,loss and accuracy might feel like you are training this model in Keras framework.\n",
    "\n",
    "Training this model in my local system took a lot of time as I didn’t have any GPU installed. I used my mac m2 only for training purpose which you can"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Load and do some Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the pre-trained BERT model and tokenizer\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the trained model weights from the `best_model.bin` file\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mclassifier_model\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./best_model_state0.6049382716049383.bin\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)))  \u001b[38;5;66;03m# Load model weights\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[1;32m      7\u001b[0m classifier_model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained BERT model and tokenizer\n",
    "\n",
    "# Load the trained model weights from the `best_model.bin` file\n",
    "classifier_model.load_state_dict(torch.load('./best_model_state0.6049382716049383.bin', map_location=torch.device('cpu')))  # Load model weights\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "classifier_model.eval()\n",
    "\n",
    "sentence = 'I do not like this movie'\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "encoding = tokenizer.encode_plus(\n",
    "    tokens,\n",
    "    max_length=50,\n",
    "    add_special_tokens=True,\n",
    "    pad_to_max_length=True,\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = classifier_model(encoding['input_ids'], encoding['attention_mask'])\n",
    "    # logits = outputs.logits\n",
    "    predicted_labels = torch.argmax(outputs, dim=1)\n",
    "\n",
    "# Convert predicted labels to human-readable sentiment labels\n",
    "sentiment_labels = ['Negative', 'Positive']\n",
    "predicted_sentiment = sentiment_labels[predicted_labels.item()]\n",
    "\n",
    "print(\"Predicted sentiment:\", predicted_sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
